# Port-Mapping für die API (HostPort:ContainerPort=8080)
HAUSKI_API_PORT=8080

# Logging-Detailgrad
RUST_LOG=info,hauski=debug

# Optional: Pfad zur Models-Konfiguration (Repo liefert eine Standarddatei)
# HAUSKI_MODELS=./configs/models.yml
# Feature-Flags (safe_mode, chat_upstream_url, ...)
# HAUSKI_FLAGS=./configs/flags.yaml
# Optional: lokaler Chat-Upstream (z. B. llama.cpp --server unter Port 8081)
# Wird normalerweise in configs/flags.yaml gesetzt.
# CHAT_UPSTREAM_URL=http://127.0.0.1:8081

# Start-all Defaults (scripts/start-all.sh)
# (Lege deine .env im Repo-Root an oder nutze configs/.env)
# Pfad zu deinem GGUF-Modell (für llama.cpp --server)
# MODEL="$HOME/models/your-model.gguf"
# Standardport für llama.cpp --server
# PORT=8081
# (scripts/stop-all.sh nutzt diesen Wert, wenn kein Port-Argument übergeben wird.)
# Explizite Upstream-URL (überschreibt Host/Port)
# Externe URLs deaktivieren automatisch den lokalen llama.cpp-Start.
# UPSTREAM_URL="http://127.0.0.1:8081"
# tmux-/Upstream-Flags
# USE_TMUX=1
# NO_UPSTREAM=1
